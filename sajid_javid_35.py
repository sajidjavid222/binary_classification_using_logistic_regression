# -*- coding: utf-8 -*-
"""sajid-javid-35.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RDM8h4dAmtaISld3bjnjAfKUmGl24H1t
"""

# Importing packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.model_selection import train_test_split

"""# Fill in the missing code
The following script is an implementation of a logistic regression model for a binary classification problem. The dataset contains four numerical input attributes and one output attribute (class label 0 or 1).

- Columns **Attribute1**, **Attribute2**, **Attribute3**, and **Attribute4** are inputs.
- Column **OutputClass** is output.

The missing pieces of code are indicated like so: `#write your code here#`.

The functions corresponding to said missing code are stated in the description above each function.

To complete the task, replace `#write your code here#` with the your own code. Also, ensure that the program executes without any warnings or errors.
"""

# Reading CSV file as pandas dataframe
data = pd.read_csv("/kaggle/input/summer-internship-cistup-iisc-2023/Full_Data.csv")

data_X = data.values[:, :-1]
data_Y = data.values[:, -1].astype("int")

# Splitting dataset into training and test set
X_full, test_X_full, Y, test_Y = train_test_split(data_X, data_Y, test_size=0.15, shuffle=True, random_state=1)

"""Try out different input column indexes as input data. For instance, if you choose to use input columns 2 and 3; replace `#write your code here#` with `1, 2` (resp.) in the code block below.

Example:
* `attr1 = 1`
* `attr2 = 2`

Note column 1 (i.e., column index = 0) is `RowIdx`, and should not be used as an input.

Also note, *column index = column number - 1*.
"""

# Removing all columns other than two
attr1 = 3
#write your code here#
attr2 = 4
#write your code here#
X = X_full[:, (attr1, attr2)]
test_X = test_X_full[:, (attr1, attr2)]

# Ploting attributes
plt.plot(X[:, 0][Y==0], X[:, 1][Y==0], "o")
plt.plot(X[:, 0][Y==1], X[:, 1][Y==1], "s")
plt.xlabel("Attribute 1")
plt.ylabel("Attribute 2")

"""The code block below constructs $\mathbf{X}$. Note that $\mathbf{X}$ includes $x_0 = 1$ column for the bias (i.e., intercept)."""

# Adding new column (i.e., first column) in input data variable; the new column store the value for bias (set to 1 at the beginning)
X = np.hstack((np.ones((X.shape[0], 1)), X))
print("The regenerated input data has class labels set to 1:\n", X)

"""## Logistic Regression Model

For Logistic Regression, our hypothesis is
$$
\hat{Y} = h_w(x) = \frac{1}{1+e^{-(w^{T}x)}}
$$
The output range of $\hat{Y}$ is between 0 and 1.

### Sigmoid Function

The Sigmoid function squishes all its inputs (i.e., values on x-axis) between 0 and 1.
$$
\sigma(z) = \frac{1}{1+e^{-z}}
$$
"""

# Defining sigmoid function
def sigmoid(z):
    # z --> input
    # sigmoid_z --> output of sigmoid function
    z = z.astype(float)
    sigmoid_z = 1/(1+np.exp(-z))
    #write your code here#

    return sigmoid_z

"""The cost function for Logistic Regression for binary classification:
$$
J(data, w) = \frac{1}{n}\sum_{i=1}^{n} L(\hat{Y}^{(i)},Y^{(i)}) = -\frac{1}{n}\sum_{i=1}^{n} [Y^{(i)}log(\hat{Y}^{(i)}) + (1-Y^{(i)})log(1-\hat{Y}^{(i)})]
$$

This loss is also called binary cross entropy error.
"""

# Defining loss function
def loss(Y, y_hat):
    # Y --> data
    # y_hat --> w
    loss = np.sum(((np.log(sigmoid(y_hat)))*Y + np.log(1-sigmoid(y_hat))*(1-Y))) / -Y.shape[0]
    #write your code here#

    return -loss

"""### Gradient of the loss function

Using the Gradient Descent Algorithm, optimal values of the parameters can be calculated like so ($\eta$ →learning rate), the update rules for parameters are as follows:
$$
w_{t+1} = w_{t} - \eta*dw
$$
Where $dw$ is the partial derivative of loss w.r.t parameter $w$. It looks like:
$$
dw = \frac{1}{n} * (\hat{y}-y).\textbf{X}
$$
"""

# Defining gradient function
def gradients(X, Y, y_hat):
    # X --> input
    # Y --> true/target value
    # y_hat --> hypothesis/predictions
    # n --> number of training examples

    n = X.shape[0]

    # Gradient of loss w.r.t weights
    dw = np.dot(X.T, (y_hat - Y)) / n
    #write your code here#

    return dw

"""Normalize the data before using/computing gradient. It can accelerate the training process. Make sure you don"t normalize the "bias" term (i.e., first column)."""

# Defining data normalization function
def normalize(X):
    # X --> input
    # n --> number of training examples
    # d --> number of features
    n, d = X.shape

    # Normalizing all the d features of X (except the bias (first) column)
    for i in range(d-1):
        X[:,i+1] = (X[:,i+1] - X[:,i+1].mean(axis=0))/X[:,i+1].std(axis=0)

    return X

"""### Prediction

Now that the functions to learn the parameters are ready, check if the hypothesis ($\hat{Y}$) is able to predict the output class $Y=1$ or $Y=0$. Note that the hypothesis is the probability of $Y$ being 1 given $\textbf{X}$ and is parameterized by $w$.

Hence, the prediction function will be so —
$$
\hat{Y} = 1 \to w^{T}\textbf{X}
\geq 0
$$
$$
\hat{Y} = 0  \to w^{T}\textbf{X} < 0
$$
"""

# Defining prediction function
def predict(X,w):
    # X --> Input.

    # Normalizing the inputs.
    X = normalize(X)

    # Calculating prediction/y_hat.
    preds = sigmoid(np.dot(X, w))

    # Empty List to store predictions.
    pred_class = []

    """
    # Loop over the predictions
    for i in range(len(preds)):
        if preds[i] >= 0.5:
            pred_class.append(1)
        else:
            pred_class.append(0)
    """
    pred_class = list(map(lambda x: 1 if x >= 0.5 else 0, preds))

    return np.array(pred_class)

"""The decision boundary will be:
$$
\hat{Y} = 0.5 \quad or \quad w^{T}\textbf{X} = 0
$$
"""

# Defining function to plot decision boundary
def plot_decision_boundary(X,w):
    ydisp = -(w[0] + w[1] * X)/w[2]

    fig = plt.figure(figsize=(10, 8))
    plt.plot(X[:, 1][Y==0], X[:, 2][Y==0], "^")
    plt.plot(X[:, 1][Y==1], X[:, 2][Y==1], "s")

    plt.xlim([-2, 5])
    plt.ylim([-2, 5])
    plt.xlabel("Attribute 1")
    plt.ylabel("Attribute 2")
    plt.title("Decision Boundary")
    plt.plot(X, ydisp)

"""Now that all the required blocks for logistic regression model are ready, encode the model."""

# Defining training function
def train(X, Y, epochs, eta):
    # X --> input
    # Y --> true/target value
    # bs --> batch size
    # eta --> learning rate
    # n-> number of training examples
    # d-> number of features

    n, d = X.shape

    # Initializing weights and bias to zeros
    w = np.zeros((d,1))

    # Reshaping Y
    Y = Y.reshape(n,1)

    # Normalizing the inputs
    X = normalize(X)

    # Empty list to store losses
    losses = []

    # Training loop
    for epoch in range(epochs):

            # Calculating hypothesis/prediction
            y_hat = sigmoid(np.dot(X, w))
            #write your code here#

            # Getting the gradients of loss w.r.t parameters
            dw = gradients(X, Y, y_hat)
            #write your code here#

            # Updating the parameters.
            w = w- eta*dw

            # Calculating loss and appending it in the list
            l = loss(Y, y_hat)
            #write your code here#
            losses.append(l)

    # Returning weights, losses(List)
    return w, losses

"""Train the model and print the results.

Try out different learning rates to improve model performance.

Example: `w, l = train(X, Y, epochs=100, eta=0.001)`
"""

# Training model
w, l = train(X, Y, epochs=100, eta=88)

# Plotting loss vs. epoch function
plt.plot(l)
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.show()

# Printing training accuracy
print("The accuracy of model is",(np.sum(1*(Y==predict(X,w)))/len(Y))*100,"%")

# Plotting the decision boundary
plot_decision_boundary(X, w)

"""Run the test data through the trained model, and print the testing accuracy."""

# Checking test accuracy
test_X = np.hstack((np.ones((test_X.shape[0],1)), test_X))
ml_predictions = predict(test_X,w)
print("The test accuracy of model is",(np.sum(1*(test_Y==ml_predictions))/len(test_Y))*100,"%")

# Exporting results
row_idx = np.concatenate((test_X_full[:, 0].astype(int), X_full[:, 0].astype(int)))
opt_cls = np.concatenate((ml_predictions.astype(int), Y.astype(int)))

output_data = np.concatenate((row_idx.reshape(-1,1), opt_cls.reshape(-1,1)), axis=1)
pd.DataFrame(output_data, columns=["RowIdx", "OutputClass"]).to_csv("Output_Data.csv", index=False)